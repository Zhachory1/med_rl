{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based RL\n",
    "\n",
    "```\n",
    "Environment -> Model Network -> Policy Network -\n",
    "     ^________________________________________ |\n",
    "```\n",
    "\n",
    "Environment\n",
    "    Real world\n",
    "    No Train\n",
    "    Outputs a state (Represents the current environment)\n",
    "    Takes in an action\n",
    "        Outputs a new state (Environment that reacted to that action)\n",
    "        \n",
    "Model Network\n",
    "    Learned world\n",
    "    Training to mimic real world\n",
    "    Takes in a state and action\n",
    "    Outputs a new state \n",
    "    \n",
    "Policy Network\n",
    "    Learned action making\n",
    "    Training to take action with best reward\n",
    "    Inputs state (be it from the model or real world)\n",
    "    Output action\n",
    "\n",
    "Let's start programming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Cause I like the distinction between xrange and range\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make some Harry Potters! (HP -> Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "H             = 8     # Hidden Layers nuerons\n",
    "learning_rate = 1e-2\n",
    "gamma         = 0.99  # Discount factor\n",
    "decay_rate    = 0.99  # For RMSProb Leaky sum of grad^2\n",
    "resume        = False # resume from previous checkpoint\n",
    "model_bs      = 3     # Batch size from model\n",
    "real_bs       = 3     # Batch size from real environment\n",
    "D             = 4     # input dimensions\n",
    "xi = tf.contrib.layers.xavier_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# Infer\n",
    "with tf.variable_scope(\"policy/infer\"):\n",
    "    observations = tf.placeholder(tf.float32, [None, 4], name=\"input_x\")\n",
    "    W1 = tf.get_variable(\"W1\", shape=[4, H], initializer=xi())\n",
    "    layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "    W2 = tf.get_variable(\"W2\", shape=[H, 1], initializer=xi())\n",
    "    score = tf.matmul(layer1, W2)\n",
    "    prob = tf.nn.sigmoid(score)\n",
    "\n",
    "# Train\n",
    "with tf.variable_scope(\"policy/train\"):\n",
    "    tvars= tf.trainable_variables()\n",
    "    input_y = tf.placeholder(tf.float32, [None,1], name=\"input_y\")\n",
    "    advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "    adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    W1Grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
    "    W2Grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "    batch_grad = [W1Grad, W2Grad]\n",
    "    loglik = tf.log(input_y*(input_y-prob) + (1 - input_y)*(input_y+prob))\n",
    "    loss = -tf.reduce_mean(loglik * advantages)\n",
    "    newGrads = tf.gradients(loss, tvars)\n",
    "    updateGrads = adam.apply_gradients(zip(batch_grad, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a 2 layer network (5->256->256->5)\n",
    "mH = 256\n",
    "\n",
    "with tf.variable_scope(\"model/infer\"):\n",
    "    prev_state = tf.placeholder(tf.float32, [None, 5], name=\"prev_state\")\n",
    "    with tf.variable_scope(\"L1\"):\n",
    "        W1M = tf.get_variable(\"W1M\", shape=[5, mH], initializer=xi())\n",
    "        b1M = tf.Variable(tf.zeros([mH]), name=\"b1M\")\n",
    "        layer1M = tf.nn.relu(tf.matmul(prev_state, W1M) + b1M)\n",
    "    with tf.variable_scope(\"L2\"):\n",
    "        W2M = tf.get_variable(\"W2M\", shape=[mH, mH], initializer=xi())\n",
    "        b2M = tf.Variable(tf.zeros([mH]), name=\"b2M\")\n",
    "        layer2M = tf.nn.relu(tf.matmul(layer1M, W2M) + b2M)\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        wO = tf.get_variable(\"wO\", shape=[mH, 4], initializer=xi())\n",
    "        wR = tf.get_variable(\"wR\", shape=[mH, 1], initializer=xi())\n",
    "        wD = tf.get_variable(\"wD\", shape=[mH, 1], initializer=xi())\n",
    "        \n",
    "        bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "        bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "        bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "        predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "        predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "        predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "with tf.variable_scope(\"model/train\"):\n",
    "    true_observation = tf.placeholder(tf.float32, [None, 4], name=\"true_observation\")\n",
    "    true_reward = tf.placeholder(tf.float32, [None, 1], name=\"true_reward\")\n",
    "    true_done = tf.placeholder(tf.float32, [None, 1], name=\"true_done\")\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        predicted_state = tf.concat([predicted_observation, \n",
    "                                     predicted_reward, \n",
    "                                     predicted_done], 1)\n",
    "        observation_loss = tf.squared_difference(true_observation, predicted_observation)\n",
    "        reward_loss = tf.squared_difference(true_reward, predicted_reward)\n",
    "        done_loss = tf.squared_difference(true_done, predicted_done)\n",
    "        \n",
    "        model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "    \n",
    "    with tf.variable_scope(\"optimize\"):\n",
    "        modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "        \n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous \n",
    "# state and action.\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],\n",
    "                         feed_dict={prev_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Policy and the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 4.000000. Reward 23.666667. action: 0.000000. mean reward 23.666667.\n",
      "World Perf: Episode 7.000000. Reward 22.666667. action: 1.000000. mean reward 23.656667.\n",
      "World Perf: Episode 10.000000. Reward 14.000000. action: 0.000000. mean reward 23.560100.\n",
      "World Perf: Episode 13.000000. Reward 39.000000. action: 0.000000. mean reward 23.714499.\n",
      "World Perf: Episode 16.000000. Reward 31.666667. action: 0.000000. mean reward 23.794021.\n",
      "World Perf: Episode 19.000000. Reward 31.666667. action: 0.000000. mean reward 23.872747.\n",
      "World Perf: Episode 22.000000. Reward 17.000000. action: 0.000000. mean reward 23.804020.\n",
      "World Perf: Episode 25.000000. Reward 30.000000. action: 0.000000. mean reward 23.865979.\n",
      "World Perf: Episode 28.000000. Reward 20.333333. action: 0.000000. mean reward 23.830653.\n",
      "World Perf: Episode 31.000000. Reward 25.333333. action: 0.000000. mean reward 23.845680.\n",
      "World Perf: Episode 34.000000. Reward 31.666667. action: 0.000000. mean reward 23.923890.\n",
      "World Perf: Episode 37.000000. Reward 22.666667. action: 1.000000. mean reward 23.911317.\n",
      "World Perf: Episode 40.000000. Reward 25.333333. action: 1.000000. mean reward 23.925538.\n",
      "World Perf: Episode 43.000000. Reward 13.000000. action: 1.000000. mean reward 23.816282.\n",
      "World Perf: Episode 46.000000. Reward 20.333333. action: 0.000000. mean reward 23.781453.\n",
      "World Perf: Episode 49.000000. Reward 24.666667. action: 0.000000. mean reward 23.790305.\n",
      "World Perf: Episode 52.000000. Reward 23.666667. action: 0.000000. mean reward 23.789068.\n",
      "World Perf: Episode 55.000000. Reward 20.333333. action: 0.000000. mean reward 23.754511.\n",
      "World Perf: Episode 58.000000. Reward 21.333333. action: 0.000000. mean reward 23.730299.\n",
      "World Perf: Episode 61.000000. Reward 32.333333. action: 1.000000. mean reward 23.816330.\n",
      "World Perf: Episode 64.000000. Reward 17.666667. action: 0.000000. mean reward 23.754833.\n",
      "World Perf: Episode 67.000000. Reward 27.000000. action: 0.000000. mean reward 23.787285.\n",
      "World Perf: Episode 70.000000. Reward 31.666667. action: 1.000000. mean reward 23.866079.\n",
      "World Perf: Episode 73.000000. Reward 23.000000. action: 1.000000. mean reward 23.857418.\n",
      "World Perf: Episode 76.000000. Reward 24.333333. action: 1.000000. mean reward 23.862177.\n",
      "World Perf: Episode 79.000000. Reward 23.333333. action: 0.000000. mean reward 23.856889.\n",
      "World Perf: Episode 82.000000. Reward 24.666667. action: 0.000000. mean reward 23.864986.\n",
      "World Perf: Episode 85.000000. Reward 30.333333. action: 0.000000. mean reward 23.929670.\n",
      "World Perf: Episode 88.000000. Reward 24.000000. action: 1.000000. mean reward 23.930373.\n",
      "World Perf: Episode 91.000000. Reward 29.333333. action: 1.000000. mean reward 23.984403.\n",
      "World Perf: Episode 94.000000. Reward 21.666667. action: 0.000000. mean reward 23.961225.\n",
      "World Perf: Episode 97.000000. Reward 54.333333. action: 1.000000. mean reward 24.264946.\n",
      "World Perf: Episode 100.000000. Reward 18.333333. action: 1.000000. mean reward 24.205630.\n",
      "World Perf: Episode 103.000000. Reward 26.000000. action: 1.000000. mean reward 24.223574.\n",
      "World Perf: Episode 106.000000. Reward 23.666667. action: 1.000000. mean reward 27.109940.\n",
      "World Perf: Episode 109.000000. Reward 19.333333. action: 0.000000. mean reward 29.926702.\n",
      "World Perf: Episode 112.000000. Reward 39.333333. action: 0.000000. mean reward 32.713390.\n",
      "World Perf: Episode 115.000000. Reward 25.333333. action: 0.000000. mean reward 35.220715.\n",
      "World Perf: Episode 118.000000. Reward 15.666667. action: 0.000000. mean reward 37.744652.\n",
      "World Perf: Episode 121.000000. Reward 34.333333. action: 0.000000. mean reward 40.455597.\n",
      "World Perf: Episode 124.000000. Reward 15.333333. action: 1.000000. mean reward 42.852570.\n",
      "World Perf: Episode 127.000000. Reward 25.000000. action: 0.000000. mean reward 45.478825.\n",
      "World Perf: Episode 130.000000. Reward 36.000000. action: 0.000000. mean reward 47.797516.\n",
      "World Perf: Episode 133.000000. Reward 27.666667. action: 1.000000. mean reward 50.008789.\n",
      "World Perf: Episode 136.000000. Reward 25.666667. action: 1.000000. mean reward 52.445938.\n",
      "World Perf: Episode 139.000000. Reward 36.000000. action: 0.000000. mean reward 54.757965.\n",
      "World Perf: Episode 142.000000. Reward 17.666667. action: 0.000000. mean reward 56.714783.\n",
      "World Perf: Episode 145.000000. Reward 19.666667. action: 1.000000. mean reward 58.752552.\n",
      "World Perf: Episode 148.000000. Reward 23.000000. action: 1.000000. mean reward 60.969791.\n",
      "World Perf: Episode 151.000000. Reward 35.000000. action: 0.000000. mean reward 63.123302.\n",
      "World Perf: Episode 154.000000. Reward 24.000000. action: 1.000000. mean reward 64.954575.\n",
      "World Perf: Episode 157.000000. Reward 44.333333. action: 1.000000. mean reward 67.045280.\n",
      "World Perf: Episode 160.000000. Reward 27.333333. action: 0.000000. mean reward 69.020950.\n",
      "World Perf: Episode 163.000000. Reward 20.333333. action: 0.000000. mean reward 70.817833.\n",
      "World Perf: Episode 166.000000. Reward 39.333333. action: 1.000000. mean reward 72.640205.\n",
      "World Perf: Episode 169.000000. Reward 39.666667. action: 1.000000. mean reward 74.449852.\n",
      "World Perf: Episode 172.000000. Reward 23.333333. action: 0.000000. mean reward 76.221611.\n",
      "World Perf: Episode 175.000000. Reward 36.000000. action: 0.000000. mean reward 78.135429.\n",
      "World Perf: Episode 178.000000. Reward 31.000000. action: 1.000000. mean reward 79.774933.\n",
      "World Perf: Episode 181.000000. Reward 29.666667. action: 1.000000. mean reward 81.503174.\n",
      "World Perf: Episode 184.000000. Reward 40.333333. action: 1.000000. mean reward 83.423531.\n",
      "World Perf: Episode 187.000000. Reward 26.333333. action: 0.000000. mean reward 85.005463.\n",
      "World Perf: Episode 190.000000. Reward 17.333333. action: 0.000000. mean reward 86.415123.\n",
      "World Perf: Episode 193.000000. Reward 15.333333. action: 0.000000. mean reward 87.919624.\n",
      "World Perf: Episode 196.000000. Reward 31.000000. action: 1.000000. mean reward 89.579216.\n",
      "World Perf: Episode 199.000000. Reward 19.666667. action: 1.000000. mean reward 90.954414.\n",
      "World Perf: Episode 202.000000. Reward 16.000000. action: 1.000000. mean reward 89.547874.\n",
      "World Perf: Episode 205.000000. Reward 35.000000. action: 1.000000. mean reward 91.190277.\n",
      "World Perf: Episode 208.000000. Reward 33.000000. action: 0.000000. mean reward 92.791168.\n",
      "World Perf: Episode 211.000000. Reward 29.666667. action: 1.000000. mean reward 91.593269.\n",
      "World Perf: Episode 214.000000. Reward 23.333333. action: 1.000000. mean reward 90.209869.\n",
      "World Perf: Episode 217.000000. Reward 28.333333. action: 1.000000. mean reward 88.881981.\n",
      "World Perf: Episode 220.000000. Reward 28.666667. action: 1.000000. mean reward 87.510674.\n",
      "World Perf: Episode 223.000000. Reward 29.000000. action: 0.000000. mean reward 86.706207.\n",
      "World Perf: Episode 226.000000. Reward 23.666667. action: 1.000000. mean reward 85.319366.\n",
      "World Perf: Episode 229.000000. Reward 26.333333. action: 0.000000. mean reward 85.936501.\n",
      "World Perf: Episode 232.000000. Reward 19.333333. action: 1.000000. mean reward 84.508354.\n",
      "World Perf: Episode 235.000000. Reward 41.333333. action: 1.000000. mean reward 85.562614.\n",
      "World Perf: Episode 238.000000. Reward 27.333333. action: 0.000000. mean reward 85.460960.\n",
      "World Perf: Episode 241.000000. Reward 34.000000. action: 1.000000. mean reward 84.221703.\n",
      "World Perf: Episode 244.000000. Reward 35.333333. action: 1.000000. mean reward 83.044952.\n",
      "World Perf: Episode 247.000000. Reward 41.666667. action: 0.000000. mean reward 82.055992.\n",
      "World Perf: Episode 250.000000. Reward 48.333333. action: 0.000000. mean reward 81.019463.\n",
      "World Perf: Episode 253.000000. Reward 20.333333. action: 0.000000. mean reward 79.909355.\n",
      "World Perf: Episode 256.000000. Reward 32.000000. action: 1.000000. mean reward 78.712425.\n",
      "World Perf: Episode 259.000000. Reward 38.000000. action: 0.000000. mean reward 77.627144.\n",
      "World Perf: Episode 262.000000. Reward 26.000000. action: 0.000000. mean reward 76.452408.\n",
      "World Perf: Episode 265.000000. Reward 31.333333. action: 0.000000. mean reward 75.338173.\n",
      "World Perf: Episode 268.000000. Reward 42.333333. action: 1.000000. mean reward 74.435966.\n",
      "World Perf: Episode 271.000000. Reward 40.333333. action: 1.000000. mean reward 73.770393.\n",
      "World Perf: Episode 274.000000. Reward 23.333333. action: 0.000000. mean reward 72.625572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 277.000000. Reward 39.666667. action: 1.000000. mean reward 71.930222.\n",
      "World Perf: Episode 280.000000. Reward 24.666667. action: 0.000000. mean reward 70.976082.\n",
      "World Perf: Episode 283.000000. Reward 35.666667. action: 1.000000. mean reward 70.048447.\n",
      "World Perf: Episode 286.000000. Reward 31.000000. action: 1.000000. mean reward 69.360924.\n",
      "World Perf: Episode 289.000000. Reward 27.333333. action: 1.000000. mean reward 68.402664.\n",
      "World Perf: Episode 292.000000. Reward 19.000000. action: 1.000000. mean reward 67.332428.\n",
      "World Perf: Episode 295.000000. Reward 46.000000. action: 0.000000. mean reward 66.577332.\n",
      "World Perf: Episode 298.000000. Reward 40.666667. action: 1.000000. mean reward 65.916489.\n",
      "World Perf: Episode 301.000000. Reward 38.333333. action: 1.000000. mean reward 65.107307.\n",
      "World Perf: Episode 304.000000. Reward 47.000000. action: 0.000000. mean reward 64.400810.\n",
      "World Perf: Episode 307.000000. Reward 16.333333. action: 1.000000. mean reward 63.458546.\n",
      "World Perf: Episode 310.000000. Reward 30.333333. action: 1.000000. mean reward 62.558578.\n",
      "World Perf: Episode 313.000000. Reward 32.666667. action: 1.000000. mean reward 61.749081.\n",
      "World Perf: Episode 316.000000. Reward 35.000000. action: 0.000000. mean reward 61.104237.\n",
      "World Perf: Episode 319.000000. Reward 26.000000. action: 0.000000. mean reward 60.293591.\n",
      "World Perf: Episode 322.000000. Reward 38.333333. action: 1.000000. mean reward 59.723038.\n",
      "World Perf: Episode 325.000000. Reward 28.333333. action: 0.000000. mean reward 59.196377.\n",
      "World Perf: Episode 328.000000. Reward 43.666667. action: 0.000000. mean reward 58.549839.\n",
      "World Perf: Episode 331.000000. Reward 25.333333. action: 0.000000. mean reward 57.923035.\n",
      "World Perf: Episode 334.000000. Reward 23.666667. action: 0.000000. mean reward 57.055161.\n",
      "World Perf: Episode 337.000000. Reward 35.666667. action: 1.000000. mean reward 56.411530.\n",
      "World Perf: Episode 340.000000. Reward 45.666667. action: 0.000000. mean reward 55.959442.\n",
      "World Perf: Episode 343.000000. Reward 41.000000. action: 0.000000. mean reward 55.341503.\n",
      "World Perf: Episode 346.000000. Reward 44.333333. action: 1.000000. mean reward 54.789646.\n",
      "World Perf: Episode 349.000000. Reward 34.666667. action: 0.000000. mean reward 54.222279.\n",
      "World Perf: Episode 352.000000. Reward 17.666667. action: 0.000000. mean reward 53.457230.\n",
      "World Perf: Episode 355.000000. Reward 37.666667. action: 1.000000. mean reward 52.868271.\n",
      "World Perf: Episode 358.000000. Reward 35.333333. action: 0.000000. mean reward 52.319534.\n",
      "World Perf: Episode 361.000000. Reward 47.666667. action: 1.000000. mean reward 51.903690.\n",
      "World Perf: Episode 364.000000. Reward 52.000000. action: 0.000000. mean reward 51.687881.\n",
      "World Perf: Episode 367.000000. Reward 28.333333. action: 0.000000. mean reward 51.405258.\n",
      "World Perf: Episode 370.000000. Reward 51.000000. action: 1.000000. mean reward 51.357071.\n",
      "World Perf: Episode 373.000000. Reward 23.000000. action: 0.000000. mean reward 50.784054.\n",
      "World Perf: Episode 376.000000. Reward 29.333333. action: 0.000000. mean reward 50.491352.\n",
      "World Perf: Episode 379.000000. Reward 32.333333. action: 0.000000. mean reward 50.370342.\n",
      "World Perf: Episode 382.000000. Reward 47.000000. action: 1.000000. mean reward 50.318623.\n",
      "World Perf: Episode 385.000000. Reward 26.666667. action: 0.000000. mean reward 49.960175.\n",
      "World Perf: Episode 388.000000. Reward 28.000000. action: 1.000000. mean reward 49.455658.\n",
      "World Perf: Episode 391.000000. Reward 39.666667. action: 1.000000. mean reward 49.189968.\n",
      "World Perf: Episode 394.000000. Reward 24.666667. action: 1.000000. mean reward 48.595768.\n",
      "World Perf: Episode 397.000000. Reward 42.333333. action: 1.000000. mean reward 48.415768.\n",
      "World Perf: Episode 400.000000. Reward 64.000000. action: 0.000000. mean reward 48.236206.\n",
      "World Perf: Episode 403.000000. Reward 22.333333. action: 1.000000. mean reward 47.600266.\n",
      "World Perf: Episode 406.000000. Reward 37.666667. action: 0.000000. mean reward 47.101105.\n",
      "World Perf: Episode 409.000000. Reward 92.666667. action: 0.000000. mean reward 47.195827.\n",
      "World Perf: Episode 412.000000. Reward 39.000000. action: 0.000000. mean reward 46.840374.\n",
      "World Perf: Episode 415.000000. Reward 30.333333. action: 0.000000. mean reward 46.311157.\n",
      "World Perf: Episode 418.000000. Reward 55.000000. action: 0.000000. mean reward 46.042049.\n",
      "World Perf: Episode 421.000000. Reward 49.666667. action: 1.000000. mean reward 45.736774.\n",
      "World Perf: Episode 424.000000. Reward 68.333333. action: 0.000000. mean reward 45.624767.\n",
      "World Perf: Episode 427.000000. Reward 49.666667. action: 0.000000. mean reward 45.299896.\n",
      "World Perf: Episode 430.000000. Reward 34.666667. action: 1.000000. mean reward 46.708237.\n",
      "World Perf: Episode 433.000000. Reward 62.666667. action: 0.000000. mean reward 49.239731.\n",
      "World Perf: Episode 436.000000. Reward 30.666667. action: 1.000000. mean reward 48.611187.\n",
      "World Perf: Episode 439.000000. Reward 59.333333. action: 0.000000. mean reward 48.287903.\n",
      "World Perf: Episode 442.000000. Reward 49.666667. action: 0.000000. mean reward 47.877758.\n",
      "World Perf: Episode 445.000000. Reward 46.000000. action: 0.000000. mean reward 47.422958.\n",
      "World Perf: Episode 448.000000. Reward 50.000000. action: 1.000000. mean reward 47.017933.\n",
      "World Perf: Episode 451.000000. Reward 85.666667. action: 0.000000. mean reward 47.011959.\n",
      "World Perf: Episode 454.000000. Reward 55.000000. action: 0.000000. mean reward 46.989223.\n",
      "World Perf: Episode 457.000000. Reward 98.333333. action: 1.000000. mean reward 50.075642.\n",
      "World Perf: Episode 460.000000. Reward 55.666667. action: 1.000000. mean reward 52.688477.\n",
      "World Perf: Episode 463.000000. Reward 69.333333. action: 1.000000. mean reward 55.198978.\n",
      "World Perf: Episode 466.000000. Reward 54.333333. action: 0.000000. mean reward 58.182598.\n",
      "World Perf: Episode 469.000000. Reward 44.333333. action: 0.000000. mean reward 60.424541.\n",
      "World Perf: Episode 472.000000. Reward 97.666667. action: 1.000000. mean reward 62.153004.\n",
      "World Perf: Episode 475.000000. Reward 55.000000. action: 0.000000. mean reward 63.362442.\n",
      "World Perf: Episode 478.000000. Reward 80.333333. action: 1.000000. mean reward 65.612549.\n",
      "World Perf: Episode 481.000000. Reward 49.666667. action: 1.000000. mean reward 67.579407.\n",
      "World Perf: Episode 484.000000. Reward 105.333333. action: 0.000000. mean reward 67.401619.\n",
      "World Perf: Episode 487.000000. Reward 71.666667. action: 0.000000. mean reward 66.966652.\n",
      "World Perf: Episode 490.000000. Reward 92.666667. action: 1.000000. mean reward 68.721260.\n",
      "World Perf: Episode 493.000000. Reward 76.666667. action: 0.000000. mean reward 71.081131.\n",
      "World Perf: Episode 496.000000. Reward 75.666667. action: 0.000000. mean reward 472954011648.000000.\n",
      "World Perf: Episode 499.000000. Reward 124.666667. action: 1.000000. mean reward 463542255616.000000.\n",
      "World Perf: Episode 502.000000. Reward 120.333333. action: 1.000000. mean reward 456149663744.000000.\n",
      "World Perf: Episode 505.000000. Reward 109.666667. action: 0.000000. mean reward 447072305152.000000.\n",
      "World Perf: Episode 508.000000. Reward 114.000000. action: 1.000000. mean reward 438181953536.000000.\n",
      "World Perf: Episode 511.000000. Reward 135.000000. action: 1.000000. mean reward 429462126592.000000.\n",
      "511\n"
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or \\\n",
    "            rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(prob,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        # record reward (has to be done after we call step() to get \n",
    "        # reward for previous action)\n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) \n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, \n",
    "            # and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1,:]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:,:]\n",
    "                rewards = np.array(epr[1:,:])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "                feed_dict={prev_state: state_prevs, \n",
    "                           true_observation: state_nexts,\n",
    "                           true_done:dones,\n",
    "                           true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss,\n",
    "                                          predicted_state,\n",
    "                                          updateModel],feed_dict)\n",
    "            if trainThePolicy == True:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads,\n",
    "                                 feed_dict={observations: epx, \n",
    "                                            input_y: epy, \n",
    "                                            advantages: discounted_epr})\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    sess.run(updateGrads,\n",
    "                             feed_dict={\n",
    "                                 W1Grad: gradBuffer[0],\n",
    "                                 W2Grad: gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None \\\n",
    "                    else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print 'World Perf: Episode %f. Reward %f. action: %f. mean reward %f.'\\\n",
    "                           % (real_episodes,reward_sum/real_bs,action,running_reward/real_bs)\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        print \"Finished training\"\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes, we start alternating between training the policy\n",
    "                # from the model and training the model from the real environment.\n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "                    if trainThePolicy:\n",
    "                        print \"We are now training the policy.\"\n",
    "                    else:\n",
    "                        print \"We are now training the model.\"\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print real_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    plt.plot(pState[:,i])\n",
    "    plt.subplot(6,2,2*i+1)\n",
    "    plt.plot(state_nextsAll[:,i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
