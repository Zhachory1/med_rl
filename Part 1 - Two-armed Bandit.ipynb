{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow: Part 1 - Multi-armed Bandit\n",
    "\n",
    "Following along with this [medium blog](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149) post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to create a 4-armed bandit (mostly because 2 arm converges too fast). The pullBandit function generates a random number from a noral dist with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit that will give that positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.00000000e+00,   1.00000000e+00,   7.00000000e+00,\n",
       "          3.30000000e+01,   1.01000000e+02,   2.06000000e+02,\n",
       "          4.30000000e+02,   8.01000000e+02,   1.13100000e+03,\n",
       "          1.41300000e+03,   1.56000000e+03,   1.48300000e+03,\n",
       "          1.17300000e+03,   7.74000000e+02,   4.70000000e+02,\n",
       "          2.59000000e+02,   1.03000000e+02,   3.80000000e+01,\n",
       "          1.10000000e+01,   4.00000000e+00]),\n",
       " array([-4.17596581, -3.78105151, -3.38613722, -2.99122292, -2.59630863,\n",
       "        -2.20139433, -1.80648004, -1.41156575, -1.01665145, -0.62173716,\n",
       "        -0.22682286,  0.16809143,  0.56300573,  0.95792002,  1.35283432,\n",
       "         1.74774861,  2.14266291,  2.5375772 ,  2.9324915 ,  3.32740579,\n",
       "         3.72232009]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e91bb5490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.randn(10000)\n",
    "plt.hist(x, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List our bandits. Currently 4 is set to most often \n",
    "# give a positive reward.\n",
    "bandits = [0.2, 0, -0.2, -5]\n",
    "num_bandits = len(bandits)\n",
    "def pullBandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Forward\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "# Backward\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight) * reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this bitch! with a $\\epsilon$-greedy action decision, we will choose an action, get the reward and run our training with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for the bandits at 0: [ 0.  0.  0.  1.]\n",
      "Rewards for the bandits at 50: [ -1.  -1.   1.  46.]\n",
      "Rewards for the bandits at 100: [ -1.   0.   3.  93.]\n",
      "Rewards for the bandits at 150: [  -2.    0.    3.  142.]\n",
      "Rewards for the bandits at 200: [  -3.    1.    2.  189.]\n",
      "Rewards for the bandits at 250: [  -3.    3.    1.  236.]\n",
      "Rewards for the bandits at 300: [  -2.    3.    1.  279.]\n",
      "Rewards for the bandits at 350: [  -4.    4.    1.  324.]\n",
      "Rewards for the bandits at 400: [  -3.    4.    1.  373.]\n",
      "Rewards for the bandits at 450: [  -1.    4.    2.  416.]\n",
      "Rewards for the bandits at 500: [  -1.    1.    2.  457.]\n",
      "Rewards for the bandits at 550: [  -2.    0.    2.  505.]\n",
      "Rewards for the bandits at 600: [  -2.    1.    3.  551.]\n",
      "Rewards for the bandits at 650: [  -2.    1.    5.  599.]\n",
      "Rewards for the bandits at 700: [  -2.    1.    4.  646.]\n",
      "Rewards for the bandits at 750: [  -2.    2.    5.  692.]\n",
      "Rewards for the bandits at 800: [  -3.    3.    4.  739.]\n",
      "Rewards for the bandits at 850: [  -3.    4.    5.  787.]\n",
      "Rewards for the bandits at 900: [  -2.    3.    4.  832.]\n",
      "Rewards for the bandits at 950: [  -2.    2.    6.  875.]\n",
      "The agent thinks bandit 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits)\n",
    "e = 0.1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        reward = pullBandit(bandits[action])\n",
    "        \n",
    "        #Update the network\n",
    "        _, resp, ww = sess.run([update, responsible_weight, weights],\n",
    "                              feed_dict={reward_holder:[reward],\n",
    "                                         action_holder:[action]})\n",
    "        \n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print \"Rewards for the bandits at \" + str(i) + \": \" + str(total_reward)\n",
    "        i += 1\n",
    "    print \"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\"\n",
    "    if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "        print \"...and it was right!\"\n",
    "    else:\n",
    "        print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Bandits\n",
    "\n",
    "In the prior exercise, we worked with multi-arm bandits, which really was an action->reward problem. In a full RL problem, we also have a state to deal with. From the state, the model chooses an action that it thinks would provide the best reward (or whatever it's policy is). Along with getting a reward, the action changes the change.  \n",
    "\n",
    "We are gonna deal with something that's right in the middle. Where we have a state, we choose and action, and get a reward, BUT the action doesn't change the state.\n",
    "\n",
    "Basically we are going to have multiple bandits, and each bandit is a different state. Since each bandit will have different reward probabilities for each arm, our agent will need to learn to condition its action on the state of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([[0.2,  0, -0.2,  -5],\n",
    "                                 [0.1, -5,    1, 0.2], \n",
    "                                 [ -5,  5,    5,   5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0, self.num_bandits)\n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "We are gonna make a NN for our agent. The input will be the current state, and then we are gonna return an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size):\n",
    "        self.state_in = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        onehot = tf.one_hot(self.state_in, s_size)\n",
    "        output = tf.layers.dense(onehot, a_size, \n",
    "                                 use_bias=True, \n",
    "                                 activation=tf.nn.sigmoid)\n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        # Train shiz!\n",
    "        self.reward_h = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "        self.action_h = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        self.resp_wgt = tf.slice(self.output, self.action_h, [1])\n",
    "        \n",
    "        self.loss = -(tf.log(self.resp_wgt)*self.reward_h)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this bitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 3 bandits: [-0.25  0.    0.  ]\n",
      "Mean reward for each of the 3 bandits: [ -0.5   21.75 -31.  ]\n",
      "Mean reward for each of the 3 bandits: [  0.5   42.75 -60.  ]\n",
      "Mean reward for each of the 3 bandits: [  4.5   56.75 -84.5 ]\n",
      "Mean reward for each of the 3 bandits: [  9.25  78.   -73.  ]\n",
      "Mean reward for each of the 3 bandits: [ 13.75  95.5  -64.5 ]\n",
      "Mean reward for each of the 3 bandits: [  14.25  114.5   -56.5 ]\n",
      "Mean reward for each of the 3 bandits: [  18.5   127.5   -49.75]\n",
      "Mean reward for each of the 3 bandits: [  29.25  144.5   -42.  ]\n",
      "Mean reward for each of the 3 bandits: [  39.25  159.    -32.5 ]\n",
      "Mean reward for each of the 3 bandits: [  45.5   174.    -19.25]\n",
      "Mean reward for each of the 3 bandits: [  47.5   196.    -13.75]\n",
      "Mean reward for each of the 3 bandits: [  50.5   217.     -6.25]\n",
      "Mean reward for each of the 3 bandits: [  59.    242.5     5.25]\n",
      "Mean reward for each of the 3 bandits: [  61.75  265.25   10.25]\n",
      "Mean reward for each of the 3 bandits: [  66.75  281.     15.  ]\n",
      "Mean reward for each of the 3 bandits: [  71.75  305.     25.  ]\n",
      "Mean reward for each of the 3 bandits: [  72.75  323.25   31.75]\n",
      "Mean reward for each of the 3 bandits: [  81.5   340.5    34.75]\n",
      "Mean reward for each of the 3 bandits: [  87.25  360.     44.  ]\n",
      "The agent thinks action 2 for bandit 1 is the most promising....\n",
      "...and it was wrong!\n",
      "The agent thinks action 2 for bandit 2 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "c_Bandit = contextual_bandit()\n",
    "myAgent = agent(lr=0.001, \n",
    "                s_size=c_Bandit.num_bandits,\n",
    "                a_size=c_Bandit.num_actions)\n",
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "total_episodes = 10000\n",
    "total_reward = np.zeros([c_Bandit.num_bandits, c_Bandit.num_actions])\n",
    "e = 0.1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        s = c_Bandit.getBandit()\n",
    "        if np.random.randn(1) < e:\n",
    "            action = np.random.randint(c_Bandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action,\n",
    "                              feed_dict={myAgent.state_in:[s]})\n",
    "        reward = c_Bandit.pullArm(action)\n",
    "        \n",
    "        # Update the network\n",
    "        feed_dict = {\n",
    "            myAgent.reward_h: [reward],\n",
    "            myAgent.action_h: [action],\n",
    "            myAgent.state_in: [s]\n",
    "        }\n",
    "        _, ww = sess.run([myAgent.update, weights], feed_dict=feed_dict)\n",
    "        \n",
    "        total_reward[s, action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print \"Mean reward for each of the \" + str(c_Bandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1))\n",
    "        i+=1\n",
    "for a in range(c_Bandit.num_bandits):\n",
    "    print \"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\"\n",
    "    if np.argmax(ww[a]) == np.argmin(c_Bandit.bandits[a]):\n",
    "        print \"...and it was right!\"\n",
    "    else:\n",
    "        print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
