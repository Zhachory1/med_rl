{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow: Part 1 - Multi-armed Bandit\n",
    "\n",
    "Following along with this [medium blog](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149) post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to create a 4-armed bandit (mostly because 2 arm converges too fast). The pullBandit function generates a random number from a noral dist with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit that will give that positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2.,    2.,   21.,   60.,  159.,  311.,  595.,  963., 1338.,\n",
       "        1561., 1598., 1296.,  943.,  611.,  306.,  148.,   56.,   23.,\n",
       "           4.,    3.]),\n",
       " array([-3.97621726, -3.5771084 , -3.17799954, -2.77889068, -2.37978182,\n",
       "        -1.98067296, -1.5815641 , -1.18245524, -0.78334638, -0.38423752,\n",
       "         0.01487134,  0.4139802 ,  0.81308906,  1.21219792,  1.61130678,\n",
       "         2.01041564,  2.4095245 ,  2.80863336,  3.20774221,  3.60685107,\n",
       "         4.00595993]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.randn(10000)\n",
    "plt.hist(x, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List our bandits. Currently 4 is set to most often \n",
    "# give a positive reward.\n",
    "bandits = [0.2, 0, -0.2, -5]\n",
    "num_bandits = len(bandits)\n",
    "def pullBandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Forward\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "# Backward\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight) * reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this bitch! with a $\\epsilon$-greedy action decision, we will choose an action, get the reward and run our training with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for the bandits at 0: [1. 0. 0. 0.]\n",
      "Rewards for the bandits at 50: [ 0. -2.  2.  1.]\n",
      "Rewards for the bandits at 100: [ 0. -2. 15.  2.]\n",
      "Rewards for the bandits at 150: [ 0. -2. 21.  4.]\n",
      "Rewards for the bandits at 200: [ 1. -1. 39.  6.]\n",
      "Rewards for the bandits at 250: [ 2. -1. 48.  8.]\n",
      "Rewards for the bandits at 300: [ 1.  0. 48.  8.]\n",
      "Rewards for the bandits at 350: [ 0.  0. 50.  9.]\n",
      "Rewards for the bandits at 400: [ 0. -1. 57.  9.]\n",
      "Rewards for the bandits at 450: [ 0.  0. 73. 12.]\n",
      "Rewards for the bandits at 500: [-1. -2. 75. 13.]\n",
      "Rewards for the bandits at 550: [-1. -4. 89. 15.]\n",
      "Rewards for the bandits at 600: [ -1.  -5. 100.  15.]\n",
      "Rewards for the bandits at 650: [ -1.  -5. 103.  16.]\n",
      "Rewards for the bandits at 700: [ -1.  -4. 126.  16.]\n",
      "Rewards for the bandits at 750: [ -1.  -6. 138.  16.]\n",
      "Rewards for the bandits at 800: [ -1.  -7. 154.  17.]\n",
      "Rewards for the bandits at 850: [ -1.  -7. 162.  17.]\n",
      "Rewards for the bandits at 900: [  0.  -8. 179.  18.]\n",
      "Rewards for the bandits at 950: [  0.  -8. 175.  22.]\n",
      "The agent thinks bandit 3 is the most promising....\n",
      "...and it was wrong!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits)\n",
    "e = 0.1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        reward = pullBandit(bandits[action])\n",
    "        \n",
    "        #Update the network\n",
    "        _, resp, ww = sess.run([update, responsible_weight, weights],\n",
    "                              feed_dict={reward_holder:[reward],\n",
    "                                         action_holder:[action]})\n",
    "        \n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print \"Rewards for the bandits at \" + str(i) + \": \" + str(total_reward)\n",
    "        i += 1\n",
    "    print \"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\"\n",
    "    if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "        print \"...and it was right!\"\n",
    "    else:\n",
    "        print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Bandits\n",
    "\n",
    "In the prior exercise, we worked with multi-arm bandits, which really was an action->reward problem. In a full RL problem, we also have a state to deal with. From the state, the model chooses an action that it thinks would provide the best reward (or whatever it's policy is). Along with getting a reward, the action changes the change.  \n",
    "\n",
    "We are gonna deal with something that's right in the middle. Where we have a state, we choose and action, and get a reward, BUT the action doesn't change the state.\n",
    "\n",
    "Basically we are going to have multiple bandits, and each bandit is a different state. Since each bandit will have different reward probabilities for each arm, our agent will need to learn to condition its action on the state of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([[0.2,  0, -0.2,  -5],\n",
    "                                 [0.1, -5,    1, 0.2], \n",
    "                                 [ -5,  5,    5,   5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0, self.num_bandits)\n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "We are gonna make a NN for our agent. The input will be the current state, and then we are gonna return an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size):\n",
    "        self.state_in = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        onehot = tf.one_hot(self.state_in, s_size)\n",
    "        output = tf.layers.dense(onehot, a_size, \n",
    "                                 use_bias=True, \n",
    "                                 activation=tf.nn.sigmoid)\n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        # Train shiz!\n",
    "        self.reward_h = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "        self.action_h = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        self.resp_wgt = tf.slice(self.output, self.action_h, [1])\n",
    "        \n",
    "        self.loss = -(tf.log(self.resp_wgt)*self.reward_h)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this bitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 3 bandits: [-0.25  0.    0.  ]\n",
      "Mean reward for each of the 3 bandits: [  1.5   17.25 -32.5 ]\n",
      "Mean reward for each of the 3 bandits: [  9.    37.75 -67.  ]\n",
      "Mean reward for each of the 3 bandits: [ 13.25  55.5  -97.5 ]\n",
      "Mean reward for each of the 3 bandits: [  16.5    75.25 -125.5 ]\n",
      "Mean reward for each of the 3 bandits: [  19.     95.75 -154.  ]\n",
      "Mean reward for each of the 3 bandits: [  22.25  112.75 -185.75]\n",
      "Mean reward for each of the 3 bandits: [  21.5   130.25 -198.5 ]\n",
      "Mean reward for each of the 3 bandits: [  23.5   148.75 -193.5 ]\n",
      "Mean reward for each of the 3 bandits: [  27.25  169.   -188.  ]\n",
      "Mean reward for each of the 3 bandits: [  25.5   187.5  -183.25]\n",
      "Mean reward for each of the 3 bandits: [  26.75  205.75 -179.25]\n",
      "Mean reward for each of the 3 bandits: [  27.25  234.25 -169.25]\n",
      "Mean reward for each of the 3 bandits: [  30.    255.75 -167.5 ]\n",
      "Mean reward for each of the 3 bandits: [  38.5   274.25 -158.5 ]\n",
      "Mean reward for each of the 3 bandits: [  41.5   294.5  -153.75]\n",
      "Mean reward for each of the 3 bandits: [  43.25  314.75 -146.75]\n",
      "Mean reward for each of the 3 bandits: [  46.    333.   -132.75]\n",
      "Mean reward for each of the 3 bandits: [  48.5   355.75 -124.  ]\n",
      "Mean reward for each of the 3 bandits: [  46.75  378.25 -121.25]\n",
      "The agent thinks action 1 for bandit 1 is the most promising....\n",
      "...and it was wrong!\n",
      "The agent thinks action 2 for bandit 2 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "c_Bandit = contextual_bandit()\n",
    "myAgent = agent(lr=0.001, \n",
    "                s_size=c_Bandit.num_bandits,\n",
    "                a_size=c_Bandit.num_actions)\n",
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "total_episodes = 10000\n",
    "total_reward = np.zeros([c_Bandit.num_bandits, c_Bandit.num_actions])\n",
    "e = 0.1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        s = c_Bandit.getBandit()\n",
    "        if np.random.randn(1) < e:\n",
    "            action = np.random.randint(c_Bandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action,\n",
    "                              feed_dict={myAgent.state_in:[s]})\n",
    "        reward = c_Bandit.pullArm(action)\n",
    "        \n",
    "        # Update the network\n",
    "        feed_dict = {\n",
    "            myAgent.reward_h: [reward],\n",
    "            myAgent.action_h: [action],\n",
    "            myAgent.state_in: [s]\n",
    "        }\n",
    "        _, ww = sess.run([myAgent.update, weights], feed_dict=feed_dict)\n",
    "        \n",
    "        total_reward[s, action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print \"Mean reward for each of the \" + str(c_Bandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1))\n",
    "        i+=1\n",
    "for a in range(c_Bandit.num_bandits):\n",
    "    print \"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\"\n",
    "    if np.argmax(ww[a]) == np.argmin(c_Bandit.bandits[a]):\n",
    "        print \"...and it was right!\"\n",
    "    else:\n",
    "        print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
