{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow: Part 1 - Multi-armed Bandit\n",
    "\n",
    "Following along with this [medium blog](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149) post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to create a 4-armed bandit (mostly because 2 arm converges too fast). The pullBandit function generates a random number from a noral dist with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit that will give that positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.000e+00, 2.000e+00, 1.100e+01, 3.700e+01, 1.020e+02, 3.090e+02,\n",
       "        5.620e+02, 9.760e+02, 1.465e+03, 1.597e+03, 1.694e+03, 1.421e+03,\n",
       "        9.590e+02, 4.940e+02, 2.440e+02, 9.100e+01, 2.200e+01, 8.000e+00,\n",
       "        4.000e+00, 1.000e+00]),\n",
       " array([-4.36431365, -3.92522819, -3.48614274, -3.04705728, -2.60797182,\n",
       "        -2.16888637, -1.72980091, -1.29071545, -0.85163   , -0.41254454,\n",
       "         0.02654092,  0.46562637,  0.90471183,  1.34379729,  1.78288274,\n",
       "         2.2219682 ,  2.66105366,  3.10013912,  3.53922457,  3.97831003,\n",
       "         4.41739549]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.randn(10000)\n",
    "plt.hist(x, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List our bandits. Currently 4 is set to most often \n",
    "# give a positive reward.\n",
    "bandits = [0.2, 0, -0.2, -5]\n",
    "num_bandits = len(bandits)\n",
    "def pullBandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Forward\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "# Backward\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight) * reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this bitch! with a $\\epsilon$-greedy action decision, we will choose an action, get the reward and run our training with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for the bandits at 0: [-1.  0.  0.  0.]\n",
      "Rewards for the bandits at 50: [-3. -1. -1. 46.]\n",
      "Rewards for the bandits at 100: [-2. -1.  1. 91.]\n",
      "Rewards for the bandits at 150: [ -1.  -1.   0. 139.]\n",
      "Rewards for the bandits at 200: [ -2.   0.  -1. 184.]\n",
      "Rewards for the bandits at 250: [ -2.  -2.   0. 229.]\n",
      "Rewards for the bandits at 300: [  0.  -3.   0. 270.]\n",
      "Rewards for the bandits at 350: [  1.  -3.   0. 319.]\n",
      "Rewards for the bandits at 400: [  1.  -3.   0. 367.]\n",
      "Rewards for the bandits at 450: [  0.  -2.   0. 413.]\n",
      "Rewards for the bandits at 500: [  0.  -1.   3. 459.]\n",
      "Rewards for the bandits at 550: [ -2.   0.   3. 506.]\n",
      "Rewards for the bandits at 600: [ -3.  -1.   3. 552.]\n",
      "Rewards for the bandits at 650: [ -3.  -2.   3. 599.]\n",
      "Rewards for the bandits at 700: [ -3.  -5.   3. 644.]\n",
      "Rewards for the bandits at 750: [ -4.  -5.   5. 691.]\n",
      "Rewards for the bandits at 800: [ -3.  -4.   7. 737.]\n",
      "Rewards for the bandits at 850: [ -4.  -4.   6. 785.]\n",
      "Rewards for the bandits at 900: [ -4.  -3.   6. 826.]\n",
      "Rewards for the bandits at 950: [ -3.  -4.   7. 869.]\n",
      "The agent thinks bandit 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits)\n",
    "e = 0.1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        reward = pullBandit(bandits[action])\n",
    "        \n",
    "        #Update the network\n",
    "        _, resp, ww = sess.run([update, responsible_weight, weights],\n",
    "                              feed_dict={reward_holder:[reward],\n",
    "                                         action_holder:[action]})\n",
    "        \n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print \"Rewards for the bandits at \" + str(i) + \": \" + str(total_reward)\n",
    "        i += 1\n",
    "    print \"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\"\n",
    "    if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "        print \"...and it was right!\"\n",
    "    else:\n",
    "        print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Bandits\n",
    "\n",
    "In the prior exercise, we worked with multi-arm bandits, which really was an action->reward problem. In a full RL problem, we also have a state to deal with. From the state, the model chooses an action that it thinks would provide the best reward (or whatever it's policy is). Along with getting a reward, the action changes the change.  \n",
    "\n",
    "We are gonna deal with something that's right in the middle. Where we have a state, we choose and action, and get a reward, BUT the action doesn't change the state.\n",
    "\n",
    "Basically we are going to have multiple bandits, and each bandit is a different state. Since each bandit will have different reward probabilities for each arm, our agent will need to learn to condition its action on the state of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([[0.2,  0, -0.2,  -5],\n",
    "                                 [0.1, -5,    1, 0.2], \n",
    "                                 [ -5,  5,    5,   5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0, self.num_bandits)\n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "We are gonna make a NN for our agent. The input will be the current state, and then we are gonna return an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size):\n",
    "        self.state_in = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        onehot = tf.one_hot(self.state_in, s_size)\n",
    "        output = tf.layers.dense(onehot, a_size, \n",
    "                                 use_bias=True, \n",
    "                                 activation=tf.nn.sigmoid)\n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        # Train shiz!\n",
    "        self.reward_h = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "        self.action_h = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        self.resp_wgt = tf.slice(self.output, self.action_h, [1])\n",
    "        \n",
    "        self.loss = -(tf.log(self.resp_wgt)*self.reward_h)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this bitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 3 bandits: [ 0.    0.   -0.25]\n",
      "Mean reward for each of the 3 bandits: [ 25.5  -12.25 -29.  ]\n",
      "Mean reward for each of the 3 bandits: [ 44.75 -25.25 -67.25]\n",
      "Mean reward for each of the 3 bandits: [ 71.75 -40.75 -93.75]\n",
      "Mean reward for each of the 3 bandits: [  95.5   -51.5  -125.75]\n",
      "Mean reward for each of the 3 bandits: [ 128.    -67.5  -133.75]\n",
      "Mean reward for each of the 3 bandits: [ 151.    -72.   -124.25]\n",
      "Mean reward for each of the 3 bandits: [ 175.75  -79.5  -112.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 198.75  -87.25 -103.75]\n",
      "Mean reward for each of the 3 bandits: [ 220.25 -102.5   -97.  ]\n",
      "Mean reward for each of the 3 bandits: [ 242.25 -113.75  -89.25]\n",
      "Mean reward for each of the 3 bandits: [ 270.25 -125.    -84.  ]\n",
      "Mean reward for each of the 3 bandits: [ 292.   -128.25  -76.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 318.5  -128.25  -71.  ]\n",
      "Mean reward for each of the 3 bandits: [ 345.75 -131.5   -69.  ]\n",
      "Mean reward for each of the 3 bandits: [ 365.   -134.75  -62.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 390.5  -140.75  -51.  ]\n",
      "Mean reward for each of the 3 bandits: [ 415.   -140.    -44.75]\n",
      "Mean reward for each of the 3 bandits: [ 435.5  -146.75  -36.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 459.25 -146.25  -34.25]\n",
      "The agent thinks action 4 for bandit 1 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 3 for bandit 2 is the most promising....\n",
      "...and it was wrong!\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "c_Bandit = contextual_bandit()\n",
    "myAgent = agent(lr=0.001, \n",
    "                s_size=c_Bandit.num_bandits,\n",
    "                a_size=c_Bandit.num_actions)\n",
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "total_episodes = 10000\n",
    "total_reward = np.zeros([c_Bandit.num_bandits, c_Bandit.num_actions])\n",
    "e = 0.1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        s = c_Bandit.getBandit()\n",
    "        if np.random.randn(1) < e:\n",
    "            action = np.random.randint(c_Bandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action,\n",
    "                              feed_dict={myAgent.state_in:[s]})\n",
    "        reward = c_Bandit.pullArm(action)\n",
    "        \n",
    "        # Update the network\n",
    "        feed_dict = {\n",
    "            myAgent.reward_h: [reward],\n",
    "            myAgent.action_h: [action],\n",
    "            myAgent.state_in: [s]\n",
    "        }\n",
    "        _, ww = sess.run([myAgent.update, weights], feed_dict=feed_dict)\n",
    "        \n",
    "        total_reward[s, action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print \"Mean reward for each of the \" + str(c_Bandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1))\n",
    "        i+=1\n",
    "for a in range(c_Bandit.num_bandits):\n",
    "    print \"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\"\n",
    "    if np.argmax(ww[a]) == np.argmin(c_Bandit.bandits[a]):\n",
    "        print \"...and it was right!\"\n",
    "    else:\n",
    "        print \"...and it was wrong!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
