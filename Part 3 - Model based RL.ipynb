{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based RL\n",
    "\n",
    "```\n",
    "Environment -> Model Network -> Policy Network -\n",
    "     ^________________________________________ |\n",
    "```\n",
    "\n",
    "Environment\n",
    "    Real world\n",
    "    No Train\n",
    "    Outputs a state (Represents the current environment)\n",
    "    Takes in an action\n",
    "        Outputs a new state (Environment that reacted to that action)\n",
    "        \n",
    "Model Network\n",
    "    Learned world\n",
    "    Training to mimic real world\n",
    "    Takes in a state and action\n",
    "    Outputs a new state \n",
    "    \n",
    "Policy Network\n",
    "    Learned action making\n",
    "    Training to take action with best reward\n",
    "    Inputs state (be it from the model or real world)\n",
    "    Output action\n",
    "\n",
    "Let's start programming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Cause I like the distinction between xrange and range\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make some Harry Potters! (HP -> Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "H             = 8     # Hidden Layers nuerons\n",
    "learning_rate = 1e-2\n",
    "gamma         = 0.99  # Discount factor\n",
    "decay_rate    = 0.99  # For RMSProb Leaky sum of grad^2\n",
    "resume        = False # resume from previous checkpoint\n",
    "model_bs      = 3     # Batch size from model\n",
    "real_bs       = 3     # Batch size from real environment\n",
    "D             = 4     # input dimensions\n",
    "xi = tf.contrib.layers.xavier_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# Infer\n",
    "with tf.variable_scope(\"policy/infer\"):\n",
    "    observations = tf.placeholder(tf.float32, [None, 4], name=\"input_x\")\n",
    "    W1 = tf.get_variable(\"W1\", shape=[4, H], initializer=xi())\n",
    "    layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "    W2 = tf.get_variable(\"W2\", shape=[H, 1], initializer=xi())\n",
    "    score = tf.matmul(layer1, W2)\n",
    "    prob = tf.nn.sigmoid(score)\n",
    "\n",
    "# Train\n",
    "with tf.variable_scope(\"policy/train\"):\n",
    "    tvars= tf.trainable_variables()\n",
    "    input_y = tf.placeholder(tf.float32, [None,1], name=\"input_y\")\n",
    "    advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "    adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    W1Grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
    "    W2Grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "    batch_grad = [W1Grad, W2Grad]\n",
    "    loglik = tf.log(input_y*(input_y-prob) + (1 - input_y)*(input_y+prob))\n",
    "    loss = -tf.reduce_mean(loglik * advantages)\n",
    "    newGrads = tf.gradients(loss, tvars)\n",
    "    updateGrads = adam.apply_gradients(zip(batch_grad, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a 2 layer network (5->256->256->5)\n",
    "mH = 256\n",
    "\n",
    "with tf.variable_scope(\"model/infer\"):\n",
    "    prev_state = tf.placeholder(tf.float32, [None, 5], name=\"prev_state\")\n",
    "    with tf.variable_scope(\"L1\"):\n",
    "        W1M = tf.get_variable(\"W1M\", shape=[5, mH], initializer=xi())\n",
    "        b1M = tf.Variable(tf.zeros([mH]), name=\"b1M\")\n",
    "        layer1M = tf.nn.relu(tf.matmul(prev_state, W1M) + b1M)\n",
    "    with tf.variable_scope(\"L2\"):\n",
    "        W2M = tf.get_variable(\"W2M\", shape=[mH, mH], initializer=xi())\n",
    "        b2M = tf.Variable(tf.zeros([mH]), name=\"b2M\")\n",
    "        layer2M = tf.nn.relu(tf.matmul(layer1M, W2M) + b2M)\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        wO = tf.get_variable(\"wO\", shape=[mH, 4], initializer=xi())\n",
    "        wR = tf.get_variable(\"wR\", shape=[mH, 1], initializer=xi())\n",
    "        wD = tf.get_variable(\"wD\", shape=[mH, 1], initializer=xi())\n",
    "        \n",
    "        bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "        bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "        bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "        predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "        predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "        predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "with tf.variable_scope(\"model/train\"):\n",
    "    true_observation = tf.placeholder(tf.float32, [None, 4], name=\"true_observation\")\n",
    "    true_reward = tf.placeholder(tf.float32, [None, 1], name=\"true_reward\")\n",
    "    true_done = tf.placeholder(tf.float32, [None, 1], name=\"true_done\")\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        predicted_state = tf.concat([predicted_observation, \n",
    "                                     predicted_reward, \n",
    "                                     predicted_done], 1)\n",
    "        observation_loss = tf.squared_difference(true_observation, predicted_observation)\n",
    "        reward_loss = tf.squared_difference(true_reward, predicted_reward)\n",
    "        done_loss = tf.squared_difference(true_done, predicted_done)\n",
    "        \n",
    "        model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "    \n",
    "    with tf.variable_scope(\"optimize\"):\n",
    "        modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "        \n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous \n",
    "# state and action.\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],\n",
    "                         feed_dict={prev_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Policy and the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf: Ep. 4.000000. Reward 21.666667. mean reward 21.666667.\n",
      "Perf: Ep. 7.000000. Reward 16.333333. mean reward 21.613333.\n",
      "Perf: Ep. 10.000000. Reward 24.333333. mean reward 21.640533.\n",
      "Perf: Ep. 13.000000. Reward 22.666667. mean reward 21.650795.\n",
      "Perf: Ep. 16.000000. Reward 29.000000. mean reward 21.724287.\n",
      "Perf: Ep. 19.000000. Reward 28.666667. mean reward 21.793711.\n",
      "Perf: Ep. 22.000000. Reward 17.000000. mean reward 21.745773.\n",
      "Perf: Ep. 25.000000. Reward 17.333333. mean reward 21.701649.\n",
      "Perf: Ep. 28.000000. Reward 34.333333. mean reward 21.827966.\n",
      "Perf: Ep. 31.000000. Reward 19.000000. mean reward 21.799686.\n",
      "Perf: Ep. 34.000000. Reward 21.666667. mean reward 21.798356.\n",
      "Perf: Ep. 37.000000. Reward 22.000000. mean reward 21.800372.\n",
      "Perf: Ep. 40.000000. Reward 35.666667. mean reward 21.939035.\n",
      "Perf: Ep. 43.000000. Reward 27.333333. mean reward 21.992978.\n",
      "Perf: Ep. 46.000000. Reward 32.000000. mean reward 22.093049.\n",
      "Perf: Ep. 49.000000. Reward 20.666667. mean reward 22.078785.\n",
      "Perf: Ep. 52.000000. Reward 22.666667. mean reward 22.084664.\n",
      "Perf: Ep. 55.000000. Reward 21.666667. mean reward 22.080484.\n",
      "Perf: Ep. 58.000000. Reward 12.666667. mean reward 21.986345.\n",
      "Perf: Ep. 61.000000. Reward 13.666667. mean reward 21.903149.\n",
      "Perf: Ep. 64.000000. Reward 20.666667. mean reward 21.890784.\n",
      "Perf: Ep. 67.000000. Reward 15.000000. mean reward 21.821876.\n",
      "Perf: Ep. 70.000000. Reward 57.333333. mean reward 22.176991.\n",
      "Perf: Ep. 73.000000. Reward 13.333333. mean reward 22.088554.\n",
      "Perf: Ep. 76.000000. Reward 25.666667. mean reward 22.124335.\n",
      "Perf: Ep. 79.000000. Reward 21.000000. mean reward 22.113092.\n",
      "Perf: Ep. 82.000000. Reward 26.333333. mean reward 22.155294.\n",
      "Perf: Ep. 85.000000. Reward 19.666667. mean reward 22.130408.\n",
      "Perf: Ep. 88.000000. Reward 37.333333. mean reward 22.282437.\n",
      "Perf: Ep. 91.000000. Reward 20.000000. mean reward 22.259613.\n",
      "Perf: Ep. 94.000000. Reward 20.666667. mean reward 22.243683.\n",
      "Perf: Ep. 97.000000. Reward 18.333333. mean reward 22.204580.\n",
      "Perf: Ep. 100.000000. Reward 15.333333. mean reward 22.135867.\n",
      "Perf: Ep. 103.000000. Reward 30.333333. mean reward 22.217842.\n",
      "Perf: Ep. 106.000000. Reward 20.666667. mean reward 25.228889.\n",
      "Perf: Ep. 109.000000. Reward 13.333333. mean reward 28.160170.\n",
      "Perf: Ep. 112.000000. Reward 18.000000. mean reward 31.076548.\n",
      "Perf: Ep. 115.000000. Reward 37.666667. mean reward 34.127605.\n",
      "Perf: Ep. 118.000000. Reward 35.333333. mean reward 36.796848.\n",
      "Perf: Ep. 121.000000. Reward 15.333333. mean reward 39.175583.\n",
      "Perf: Ep. 124.000000. Reward 22.000000. mean reward 41.621288.\n",
      "Perf: Ep. 127.000000. Reward 29.000000. mean reward 44.058701.\n",
      "Perf: Ep. 130.000000. Reward 32.333333. mean reward 46.443481.\n",
      "Perf: Ep. 133.000000. Reward 21.000000. mean reward 48.730267.\n",
      "Perf: Ep. 136.000000. Reward 17.666667. mean reward 50.973820.\n",
      "Perf: Ep. 139.000000. Reward 19.333333. mean reward 53.093079.\n",
      "Perf: Ep. 142.000000. Reward 30.333333. mean reward 55.321716.\n",
      "Perf: Ep. 145.000000. Reward 27.333333. mean reward 56.574142.\n",
      "Perf: Ep. 148.000000. Reward 24.333333. mean reward 55.962738.\n",
      "Perf: Ep. 151.000000. Reward 26.000000. mean reward 57.256039.\n",
      "Perf: Ep. 154.000000. Reward 41.000000. mean reward 59.179379.\n",
      "Perf: Ep. 157.000000. Reward 33.000000. mean reward 61.330631.\n",
      "Perf: Ep. 160.000000. Reward 26.000000. mean reward 62.494183.\n",
      "Perf: Ep. 163.000000. Reward 22.333333. mean reward 62.281021.\n",
      "Perf: Ep. 166.000000. Reward 36.000000. mean reward 62.884354.\n",
      "Perf: Ep. 169.000000. Reward 41.000000. mean reward 64.159019.\n",
      "Perf: Ep. 172.000000. Reward 33.333333. mean reward 65.510689.\n",
      "Perf: Ep. 175.000000. Reward 32.333333. mean reward 65.673897.\n",
      "Perf: Ep. 178.000000. Reward 28.000000. mean reward 65.059692.\n",
      "Perf: Ep. 181.000000. Reward 36.000000. mean reward 67.407768.\n",
      "Perf: Ep. 184.000000. Reward 17.000000. mean reward 68.316689.\n",
      "Perf: Ep. 187.000000. Reward 33.333333. mean reward 70.317986.\n",
      "Perf: Ep. 190.000000. Reward 24.333333. mean reward 71.438057.\n",
      "Perf: Ep. 193.000000. Reward 30.333333. mean reward 71.392357.\n",
      "Perf: Ep. 196.000000. Reward 46.333333. mean reward 73.021065.\n",
      "Perf: Ep. 199.000000. Reward 24.000000. mean reward 75.107948.\n",
      "Perf: Ep. 202.000000. Reward 19.666667. mean reward 75.828529.\n",
      "Perf: Ep. 205.000000. Reward 25.333333. mean reward 76.417366.\n",
      "Perf: Ep. 208.000000. Reward 35.000000. mean reward 78.419571.\n",
      "Perf: Ep. 211.000000. Reward 58.333333. mean reward 80.499565.\n",
      "Perf: Ep. 214.000000. Reward 45.000000. mean reward 79.559761.\n",
      "Perf: Ep. 217.000000. Reward 36.333333. mean reward 81.412300.\n",
      "Perf: Ep. 220.000000. Reward 44.666667. mean reward 83.269409.\n",
      "Perf: Ep. 223.000000. Reward 21.666667. mean reward 84.194183.\n",
      "Perf: Ep. 226.000000. Reward 42.666667. mean reward 85.910492.\n",
      "Perf: Ep. 229.000000. Reward 33.666667. mean reward 87.607780.\n",
      "Perf: Ep. 232.000000. Reward 39.666667. mean reward 89.342842.\n",
      "Perf: Ep. 235.000000. Reward 38.333333. mean reward 91.038643.\n",
      "Perf: Ep. 238.000000. Reward 14.333333. mean reward 89.496910.\n",
      "Perf: Ep. 241.000000. Reward 33.000000. mean reward 88.248299.\n",
      "Perf: Ep. 244.000000. Reward 40.666667. mean reward 89.852081.\n",
      "Perf: Ep. 247.000000. Reward 29.000000. mean reward 89.468330.\n",
      "Perf: Ep. 250.000000. Reward 20.000000. mean reward 90.927574.\n",
      "Perf: Ep. 253.000000. Reward 46.333333. mean reward 92.547119.\n",
      "Perf: Ep. 256.000000. Reward 34.333333. mean reward 94.043121.\n",
      "Perf: Ep. 259.000000. Reward 21.333333. mean reward 95.362862.\n",
      "Perf: Ep. 262.000000. Reward 24.666667. mean reward 96.637642.\n",
      "Perf: Ep. 265.000000. Reward 46.333333. mean reward 97.293633.\n",
      "Perf: Ep. 268.000000. Reward 26.333333. mean reward 97.713631.\n",
      "Perf: Ep. 271.000000. Reward 55.000000. mean reward 97.332306.\n",
      "Perf: Ep. 274.000000. Reward 57.000000. mean reward 98.959229.\n",
      "Perf: Ep. 277.000000. Reward 27.000000. mean reward 100.203545.\n",
      "Perf: Ep. 280.000000. Reward 24.000000. mean reward 100.522240.\n",
      "Perf: Ep. 283.000000. Reward 27.000000. mean reward 100.834129.\n",
      "Perf: Ep. 286.000000. Reward 32.000000. mean reward 99.367615.\n",
      "Perf: Ep. 289.000000. Reward 24.666667. mean reward 100.600349.\n",
      "Perf: Ep. 292.000000. Reward 60.666667. mean reward 99.413788.\n",
      "Perf: Ep. 295.000000. Reward 33.000000. mean reward 100.664864.\n",
      "Perf: Ep. 298.000000. Reward 53.333333. mean reward 101.252838.\n",
      "Perf: Ep. 301.000000. Reward 21.000000. mean reward 99.593224.\n",
      "Perf: Ep. 304.000000. Reward 33.333333. mean reward 100.918671.\n",
      "Perf: Ep. 307.000000. Reward 29.333333. mean reward 101.806328.\n",
      "Perf: Ep. 310.000000. Reward 58.333333. mean reward 102.358513.\n",
      "Perf: Ep. 313.000000. Reward 34.666667. mean reward 100.816223.\n",
      "Perf: Ep. 316.000000. Reward 32.333333. mean reward 99.344429.\n",
      "Perf: Ep. 319.000000. Reward 47.000000. mean reward 100.419624.\n",
      "Perf: Ep. 322.000000. Reward 24.666667. mean reward 101.024780.\n",
      "Perf: Ep. 325.000000. Reward 31.333333. mean reward 99.612366.\n",
      "Perf: Ep. 328.000000. Reward 19.333333. mean reward 98.179077.\n",
      "Perf: Ep. 331.000000. Reward 20.666667. mean reward 96.609032.\n",
      "Perf: Ep. 334.000000. Reward 27.000000. mean reward 95.123932.\n",
      "Perf: Ep. 337.000000. Reward 49.666667. mean reward 93.925873.\n",
      "Perf: Ep. 340.000000. Reward 31.000000. mean reward 92.611328.\n",
      "Perf: Ep. 343.000000. Reward 35.000000. mean reward 91.267830.\n",
      "Perf: Ep. 346.000000. Reward 37.666667. mean reward 89.882439.\n",
      "Perf: Ep. 349.000000. Reward 36.000000. mean reward 88.681213.\n",
      "Perf: Ep. 352.000000. Reward 58.333333. mean reward 87.973938.\n",
      "Perf: Ep. 355.000000. Reward 46.666667. mean reward 86.973328.\n",
      "Perf: Ep. 358.000000. Reward 42.333333. mean reward 88.752205.\n",
      "Perf: Ep. 361.000000. Reward 46.666667. mean reward 87.926613.\n",
      "Perf: Ep. 364.000000. Reward 60.000000. mean reward 87.260826.\n",
      "Perf: Ep. 367.000000. Reward 43.333333. mean reward 86.339272.\n",
      "Perf: Ep. 370.000000. Reward 55.666667. mean reward 86.129997.\n",
      "Perf: Ep. 373.000000. Reward 30.333333. mean reward 84.829681.\n",
      "Perf: Ep. 376.000000. Reward 55.666667. mean reward 83.833565.\n",
      "Perf: Ep. 379.000000. Reward 46.666667. mean reward 82.974091.\n",
      "Perf: Ep. 382.000000. Reward 45.333333. mean reward 81.899872.\n",
      "Perf: Ep. 385.000000. Reward 32.666667. mean reward 80.937653.\n",
      "Perf: Ep. 388.000000. Reward 40.333333. mean reward 79.896759.\n",
      "Perf: Ep. 391.000000. Reward 73.333333. mean reward 79.162010.\n",
      "Perf: Ep. 394.000000. Reward 70.666667. mean reward 78.406448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf: Ep. 397.000000. Reward 49.000000. mean reward 77.584053.\n",
      "Perf: Ep. 400.000000. Reward 51.000000. mean reward 76.613747.\n",
      "Perf: Ep. 403.000000. Reward 52.000000. mean reward 75.667610.\n",
      "Perf: Ep. 406.000000. Reward 53.666667. mean reward 74.791252.\n",
      "Perf: Ep. 409.000000. Reward 71.333333. mean reward 74.311562.\n",
      "Perf: Ep. 412.000000. Reward 39.333333. mean reward 73.413445.\n",
      "Perf: Ep. 415.000000. Reward 52.666667. mean reward 72.549065.\n",
      "Perf: Ep. 418.000000. Reward 40.666667. mean reward 71.579865.\n",
      "Perf: Ep. 421.000000. Reward 55.333333. mean reward 70.952370.\n",
      "Perf: Ep. 424.000000. Reward 41.666667. mean reward 70.043388.\n",
      "Perf: Ep. 427.000000. Reward 47.000000. mean reward 69.200317.\n",
      "Perf: Ep. 430.000000. Reward 82.000000. mean reward 68.998108.\n",
      "Perf: Ep. 433.000000. Reward 66.000000. mean reward 68.387489.\n",
      "Perf: Ep. 436.000000. Reward 65.666667. mean reward 67.788269.\n",
      "Perf: Ep. 439.000000. Reward 45.333333. mean reward 67.204628.\n",
      "Perf: Ep. 442.000000. Reward 69.000000. mean reward 66.713272.\n",
      "Perf: Ep. 445.000000. Reward 63.333333. mean reward 66.991997.\n",
      "Perf: Ep. 448.000000. Reward 53.666667. mean reward 66.566002.\n",
      "Perf: Ep. 451.000000. Reward 45.333333. mean reward 65.832649.\n",
      "Perf: Ep. 454.000000. Reward 46.666667. mean reward 65.176384.\n",
      "Perf: Ep. 457.000000. Reward 92.000000. mean reward 65.051003.\n",
      "Perf: Ep. 460.000000. Reward 42.000000. mean reward 64.835083.\n",
      "Perf: Ep. 463.000000. Reward 61.666667. mean reward 64.631401.\n",
      "Perf: Ep. 466.000000. Reward 49.666667. mean reward 64.205795.\n",
      "Perf: Ep. 469.000000. Reward 40.666667. mean reward 63.517117.\n",
      "Perf: Ep. 472.000000. Reward 56.666667. mean reward 63.039524.\n",
      "Perf: Ep. 475.000000. Reward 49.666667. mean reward 62.536808.\n",
      "Perf: Ep. 478.000000. Reward 62.666667. mean reward 62.946224.\n",
      "Perf: Ep. 481.000000. Reward 69.333333. mean reward 63.452255.\n",
      "Perf: Ep. 484.000000. Reward 60.000000. mean reward 63.006237.\n",
      "Perf: Ep. 487.000000. Reward 66.000000. mean reward 62.839111.\n",
      "Perf: Ep. 490.000000. Reward 82.333333. mean reward 62.641743.\n",
      "Perf: Ep. 493.000000. Reward 76.666667. mean reward 62.352203.\n",
      "Perf: Ep. 496.000000. Reward 70.333333. mean reward 64.779945.\n",
      "Perf: Ep. 499.000000. Reward 57.333333. mean reward 67.255257.\n",
      "Perf: Ep. 502.000000. Reward 70.333333. mean reward 66.790215.\n",
      "Perf: Ep. 505.000000. Reward 62.000000. mean reward 69.202362.\n",
      "Perf: Ep. 508.000000. Reward 49.000000. mean reward 68.449310.\n",
      "Perf: Ep. 511.000000. Reward 68.333333. mean reward 68.147888.\n"
     ]
    },
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-062f63ce3169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Start displaying environment once performance is acceptably high.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdrawFromModel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m             \u001b[0mrendering\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrue\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mrendering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/gym/envs/classic_control/cartpole.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/pyglet/gl/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/pyglet/window/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1896\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/pyglet/gl/__init__.pyc\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/pyglet/window/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/pyglet/window/__init__.pyc\u001b[0m in \u001b[0;36mget_default_display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \"\"\"\n\u001b[0;32m-> 1845\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/pyglet/canvas/__init__.pyc\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/pyglet/canvas/xlib.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 100 and drawFromModel == False) or \\\n",
    "            rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(prob,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        # record reward (has to be done after we call step() to get \n",
    "        # reward for previous action)\n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) \n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, \n",
    "            # and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1,:]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:,:]\n",
    "                rewards = np.array(epr[1:,:])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "                feed_dict={prev_state: state_prevs, \n",
    "                           true_observation: state_nexts,\n",
    "                           true_done:dones,\n",
    "                           true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss,\n",
    "                                          predicted_state,\n",
    "                                          updateModel],feed_dict)\n",
    "            if trainThePolicy == True:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads,\n",
    "                                 feed_dict={observations: epx, \n",
    "                                            input_y: epy, \n",
    "                                            advantages: discounted_epr})\n",
    "                \n",
    "                # If gradients become too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    sess.run(updateGrads,\n",
    "                             feed_dict={\n",
    "                                 W1Grad: gradBuffer[0],\n",
    "                                 W2Grad: gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "                \n",
    "                if reward_sum > 1000:\n",
    "                    reward_sum = 1000\n",
    "                running_reward = reward_sum if running_reward is None \\\n",
    "                    else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print 'Perf: Ep. %f. Reward %f. mean reward %f.'\\\n",
    "                           % (real_episodes,reward_sum/real_bs,running_reward/real_bs)\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        print \"Finished training\"\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes, we start alternating between training the policy\n",
    "                # from the model and training the model from the real environment.\n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print real_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    plt.plot(pState[:,i], color=\"g\")\n",
    "    plt.subplot(6,2,2*i+1)\n",
    "    plt.plot(state_nextsAll[:,i], color='r')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
