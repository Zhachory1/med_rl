{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Based Agents\n",
    "\n",
    "This notebook will describe how we get from a simple agent to one that is\n",
    "capable of taking in an observation/state of the world, and taking actions\n",
    "which provide the optimal reward not just in the present, but over the long \n",
    "run. With this, we can have a full reinforcemnet learning agent.\n",
    "\n",
    "Environments which pose the entire problem to an agent are reffered to as \n",
    "Markov Decision Processes (MDPs). ALong with this, these rewards are not \n",
    "only tied to the actions the agent takes, but also to the state of the\n",
    "environment. So now, it's evident that the agent must make good decisions\n",
    "in the past to get more rewards in the future.\n",
    "\n",
    "To define an MDP, we say it consists of a set of all possible states `S` from which our agent can experience `s`. A set of possible actions `A` from which our agent a any time will take action `a`. Given a state-action pair `(s, a)`, the transistion probablity to a new state `s'` is defined by `T(s, a)` and the reward `r` is given by `R(s, a)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart-pole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Cause I like the distinction between xrange and range\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy-Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# r is a 1D float array of rewards and returns the computed discounted \n",
    "# reward.\n",
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_sum = running_sum * gamma + r[t]\n",
    "        discounted_r[t] = running_sum\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    \"\"\"\n",
    "        lr:     float - learning_rate\n",
    "        s_size: int   - State/input size\n",
    "        a_size: int   - Action/output size\n",
    "        h_size: int   - number of neurons in the hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, s_size, a_size, h_size):\n",
    "        self.input = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "        hidden = tf.layers.dense(self.input, \n",
    "                                 h_size, \n",
    "                                 bias_initializer=None,\n",
    "                                 activation=tf.nn.relu)\n",
    "        self.output = tf.layers.dense(hidden, \n",
    "                                      a_size,\n",
    "                                      bias_initializer=None,\n",
    "                                      activation=tf.nn.softmax)\n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "        \n",
    "        # We use these vairables to get the reward and action into the\n",
    "        # network to compute the loss, and use it to update the network.\n",
    "        self.r_holder = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.a_holder = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        self.indices = tf.range(0, tf.shape(self.output)[0]) * \\\n",
    "                       tf.shape(self.output)[1] + \\\n",
    "                       self.a_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indices)\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs) * self.r_holder)\n",
    "        tvars = tf.trainable_variables()\n",
    "        self.g_holder = []\n",
    "        for idx, var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32, name=str(idx)+'_holder')\n",
    "            self.g_holder.append(placeholder)\n",
    "            \n",
    "        self.gradients = tf.gradients(self.loss, tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.g_holder, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "15.93\n",
      "18.44\n",
      "38.02\n",
      "44.26\n",
      "57.02\n",
      "67.29\n",
      "49.97\n",
      "118.43\n",
      "172.32\n",
      "179.96\n",
      "197.44\n",
      "193.62\n",
      "196.91\n",
      "200.0\n",
      "199.0\n",
      "198.28\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "myAgent = agent(lr=1e-2, s_size=4, a_size=2, h_size=8)\n",
    "\n",
    "total_episodes = 5000\n",
    "max_ep = 999\n",
    "update_freq = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    \n",
    "    grad_buffer = sess.run(tf.trainable_variables())\n",
    "    for ix, grad in enumerate(grad_buffer):\n",
    "        grad_buffer[ix] = grad * 0\n",
    "        \n",
    "    while i < total_episodes:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_ep):\n",
    "            # Take probabilities of actions from network \n",
    "            a_dist = sess.run(myAgent.output, feed_dict={myAgent.input:[s]})\n",
    "            # Weighted random choice\n",
    "            a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "            # Grab index\n",
    "            a = np.argmax(a_dist == a)\n",
    "\n",
    "            # Do action and get reward and sate and done if not.\n",
    "            s1, r, d, _ = env.step(a)\n",
    "            ep_history.append([s,a,r,s1])\n",
    "            s = s1\n",
    "            running_reward += r\n",
    "\n",
    "            if d == True:\n",
    "                #Update this bitch, foo!\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 2] = discount_rewards(ep_history[:,2])\n",
    "                feed_dict = {\n",
    "                    myAgent.r_holder: ep_history[:,2],\n",
    "                    myAgent.a_holder: ep_history[:,1],\n",
    "                    myAgent.input: np.vstack(ep_history[:, 0])\n",
    "                }\n",
    "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    grad_buffer[idx] += grad\n",
    "\n",
    "                if i % update_freq == 0 and i != 0:\n",
    "                    feed_dict = dict(zip(myAgent.g_holder, grad_buffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix, grad in enumerate(grad_buffer):\n",
    "                        grad_buffer[ix] = grad * 0\n",
    "\n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(j)\n",
    "                break\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print np.mean(total_reward[-100:])\n",
    "        i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
